{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVq0tvZy8oMT+OOxZuXyw3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LimelightVision/notebooks/blob/main/limelight-tfrecord-unpacker-notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![UnpackerNotebookLogo](https://ik.imagekit.io/llimi/docs/UnpackerNotebookLogo.png?updatedAt=1739507557283)"
      ],
      "metadata": {
        "id": "5C9a2XUiu8c0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os\n",
        "from IPython.display import display\n",
        "from ipywidgets import Text, Button, VBox\n",
        "\n",
        "def process_dataset():\n",
        "    link_input = Text(\n",
        "        value='',\n",
        "        placeholder='Paste your Google Drive share link here',\n",
        "        description='Drive Link:',\n",
        "        style={'description_width': 'initial'},\n",
        "        layout={'width': '50%'}\n",
        "    )\n",
        "\n",
        "    def on_click(b):\n",
        "        try:\n",
        "            print(\"Downloading dataset...\")\n",
        "            url = link_input.value\n",
        "\n",
        "            if 'drive.google.com/file/d/' in url:\n",
        "                file_id = url.split('/file/d/')[1].split('/')[0]\n",
        "                url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "            output = '/content/dataset.zip'\n",
        "            gdown.download(url, output, fuzzy=True)\n",
        "            print(\"Download complete!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {str(e)}\")\n",
        "\n",
        "    process_button = Button(description='Process Dataset', button_style='primary')\n",
        "    process_button.on_click(on_click)\n",
        "    display(VBox([link_input, process_button]))\n",
        "\n",
        "# Install gdown if not already installed\n",
        "!pip install -q gdown --upgrade\n",
        "\n",
        "# Execute\n",
        "process_dataset()"
      ],
      "metadata": {
        "id": "vd6lPH2lqvcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasetPath = '/content/dataset.zip'\n",
        "print(datasetPath)\n",
        "!unzip $datasetPath\n"
      ],
      "metadata": {
        "id": "tTPB93qLrTPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uKFJgRzqcP0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import fnmatch\n",
        "\n",
        "def find_files(directory, pattern):\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for basename in files:\n",
        "            if fnmatch.fnmatch(basename, pattern):\n",
        "                filename = os.path.join(root, basename)\n",
        "                yield filename\n",
        "\n",
        "def set_tfrecord_variables(directory):\n",
        "    train_record_fname = ''\n",
        "    val_record_fname = ''\n",
        "    label_map_pbtxt_fname = ''\n",
        "\n",
        "    for tfrecord_file in find_files(directory, '*.tfrecord'):\n",
        "        if '/train/' in tfrecord_file:\n",
        "            train_record_fname = tfrecord_file\n",
        "        elif '/valid/' in tfrecord_file:\n",
        "            val_record_fname = tfrecord_file\n",
        "        elif '/test/' in tfrecord_file:\n",
        "            pass\n",
        "\n",
        "    for label_map_file in find_files(directory, '*_label_map.pbtxt'):\n",
        "        label_map_pbtxt_fname = label_map_file  # Assuming one common label map file\n",
        "\n",
        "    return train_record_fname, val_record_fname, label_map_pbtxt_fname\n",
        "\n",
        "\n",
        "train_record_fname, val_record_fname, label_map_pbtxt_fname = set_tfrecord_variables('/content')\n",
        "\n",
        "\n",
        "print(\"Train Record File:\", train_record_fname)\n",
        "print(\"Validation Record File:\", val_record_fname)\n",
        "print(\"Label Map File:\", label_map_pbtxt_fname)\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def parse_tfrecord(example_proto):\n",
        "    \"\"\"Parse the input tf.Example proto using the dictionary feature_description.\"\"\"\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/format': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    return tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "def decode_image(parsed_features):\n",
        "    \"\"\"Decode the image from parsed features.\"\"\"\n",
        "    image = tf.io.decode_image(parsed_features['image/encoded'])\n",
        "    return image\n",
        "\n",
        "def display_images(tfrecord_path, num_images=5):\n",
        "    \"\"\"Display images from a TFRecord file.\"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 3*num_images))\n",
        "\n",
        "    for i, parsed_features in enumerate(dataset.take(num_images)):\n",
        "        image = decode_image(parsed_features)\n",
        "\n",
        "        plt.subplot(num_images, 1, i+1)\n",
        "        plt.imshow(image.numpy())\n",
        "        plt.axis('off')\n",
        "        plt.title(f'Image {i+1}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def display_single_image(tfrecord_path, image_index=0):\n",
        "    \"\"\"Display a single image from a TFRecord file.\"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "    for i, parsed_features in enumerate(dataset):\n",
        "        if i == image_index:\n",
        "            image = decode_image(parsed_features)\n",
        "            plt.figure(figsize=(10, 10))\n",
        "            plt.imshow(image.numpy())\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "            break\n",
        "\n",
        "\n",
        "tfrecord_path = train_record_fname\n",
        "display_images(tfrecord_path, num_images=5)\n",
        "# Example usage for single image:\n",
        "# display_single_image(tfrecord_path, image_index=0)"
      ]
    }
  ]
}